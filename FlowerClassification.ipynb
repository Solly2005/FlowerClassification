{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qmaUlcmrvPm1"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import cv2\n",
        "import os\n",
        "import PIL\n",
        "import tensorflow as tf\n",
        "\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras.models import Sequential"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_url = \"https://storage.googleapis.com/download.tensorflow.org/example_images/flower_photos.tgz\"\n",
        "data_dir = tf.keras.utils.get_file('flower_photos', origin=dataset_url,  cache_dir='.', untar=True)"
      ],
      "metadata": {
        "id": "1-PlJksevW_I",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ae40c831-a993-472d-9132-4d91191a0a08"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/download.tensorflow.org/example_images/flower_photos.tgz\n",
            "\u001b[1m228813984/228813984\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 0us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data_dir"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "k7QrMCEKvlLV",
        "outputId": "0c24889a-176b-442a-dc77-7978261e6e7e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'./datasets/flower_photos'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pdRib-76voOo",
        "outputId": "209ba888-c36a-4314-c9d7-eefb166dc912"
      },
      "source": [
        "import pathlib\n",
        "data_dir = pathlib.Path(data_dir) / 'flower_photos'\n",
        "data_dir = pathlib.Path(data_dir)\n",
        "data_dir"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "PosixPath('datasets/flower_photos/flower_photos')"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "flowers_images_dict = {\n",
        "    'roses': list(data_dir.glob('roses/*')),\n",
        "    'daisy': list(data_dir.glob('daisy/*')),\n",
        "    'dandelion': list(data_dir.glob('dandelion/*')),\n",
        "    'sunflowers': list(data_dir.glob('sunflowers/*')),\n",
        "    'tulips': list(data_dir.glob('tulips/*')),\n",
        "}"
      ],
      "metadata": {
        "id": "mRA0QDCKvs5E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "flowers_labels_dict = {\n",
        "    'roses': 0,\n",
        "    'daisy': 1,\n",
        "    'dandelion': 2,\n",
        "    'sunflowers': 3,\n",
        "    'tulips': 4,\n",
        "}"
      ],
      "metadata": {
        "id": "YAlU447LwENa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3883a0c0"
      },
      "source": [
        "IMG_WIDTH = 180\n",
        "IMG_HEIGHT = 180"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Removed the data_augmentation sequential model definition"
      ],
      "metadata": {
        "id": "BzgQBlhPwI_W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "import numpy as np\n",
        "\n",
        "# ✅ Step 1: Prepare data lists\n",
        "all_image_paths = []\n",
        "all_image_labels = []\n",
        "\n",
        "for label in flowers_images_dict:\n",
        "    image_list = flowers_images_dict.get(label, [])\n",
        "    label_index = flowers_labels_dict.get(label)\n",
        "\n",
        "    for image_path in image_list:\n",
        "        all_image_paths.append(str(image_path))\n",
        "        all_image_labels.append(label_index)\n",
        "\n",
        "# ✅ Step 2: Check if data was loaded correctly\n",
        "print(\"Total image paths:\", len(all_image_paths))\n",
        "print(\"Total labels:\", len(all_image_labels))\n",
        "\n",
        "if len(all_image_paths) == 0:\n",
        "    raise ValueError(\"No image paths found. Please check your data dictionaries.\")\n",
        "\n",
        "# ✅ Step 3: Convert to numpy arrays\n",
        "all_image_paths = np.array(all_image_paths)\n",
        "all_image_labels = np.array(all_image_labels)\n",
        "\n",
        "# ✅ Step 4: Split the dataset\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    all_image_paths,\n",
        "    all_image_labels,\n",
        "    test_size=0.2,\n",
        "    random_state=42,\n",
        "    stratify=all_image_labels  # Ensures class balance\n",
        ")\n",
        "\n",
        "# ✅ Step 5: Output confirmation\n",
        "print(\"Training set size:\", len(X_train))\n",
        "print(\"Test set size:\", len(X_test))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ONQg81edzvmP",
        "outputId": "b0621906-d20b-4643-ac4e-618935b7f80f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total image paths: 3670\n",
            "Total labels: 3670\n",
            "Training set size: 2936\n",
            "Test set size: 734\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Removed redundant code for scaling X_train and y_train as tf.data pipeline is used for data loading and preprocessing.\n",
        "# The datasets train_ds and test_ds are created and preprocessed in cell d1360778."
      ],
      "metadata": {
        "id": "kla_2mOs8zFu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d1360778"
      },
      "source": [
        "def preprocess_image(image_path, label):\n",
        "    img = tf.io.read_file(image_path)\n",
        "    img = tf.image.decode_jpeg(img, channels=3)\n",
        "    img = tf.image.resize(img, [IMG_HEIGHT, IMG_WIDTH])\n",
        "    img = img / 255.0  # Normalize to [0, 1]\n",
        "    return img, label\n",
        "\n",
        "# Create TensorFlow Datasets from original paths and labels\n",
        "train_ds = tf.data.Dataset.from_tensor_slices((X_train, y_train))\n",
        "test_ds = tf.data.Dataset.from_tensor_slices((X_test, y_test))\n",
        "\n",
        "# Apply preprocessing\n",
        "train_ds = train_ds.map(preprocess_image, num_parallel_calls=tf.data.AUTOTUNE)\n",
        "test_ds = test_ds.map(preprocess_image, num_parallel_calls=tf.data.AUTOTUNE)\n",
        "\n",
        "\n",
        "# Batch the datasets\n",
        "train_ds = train_ds.batch(32)\n",
        "test_ds = test_ds.batch(32)\n",
        "\n",
        "# Apply data augmentation to the training dataset (assuming data_augmentation is defined elsewhere)\n",
        "# If data_augmentation is part of the model, remove this line\n",
        "# train_ds = train_ds.map(lambda x, y: (data_augmentation(x, training=True), y), num_parallel_calls=tf.data.AUTOTUNE)\n",
        "\n",
        "\n",
        "# Cache and prefetch the datasets\n",
        "train_ds = train_ds.cache().prefetch(buffer_size=tf.data.AUTOTUNE)\n",
        "test_ds = test_ds.cache().prefetch(buffer_size=tf.data.AUTOTUNE)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_classes = 5\n",
        "IMG_HEIGHT = 180\n",
        "IMG_WIDTH = 180\n",
        "\n",
        "# Build the model\n",
        "model = keras.Sequential([\n",
        "    keras.Input(shape=(IMG_HEIGHT, IMG_WIDTH, 3)),  # Input layer\n",
        "    layers.RandomFlip(\"horizontal\"), # Added data augmentation layers\n",
        "    layers.RandomRotation(0.1),\n",
        "    layers.RandomZoom(0.1),\n",
        "    layers.Conv2D(16, 3, padding='same', activation='relu', input_shape=(IMG_HEIGHT, IMG_WIDTH, 3)), # Explicitly set input shape\n",
        "    layers.MaxPooling2D(),\n",
        "    layers.Conv2D(32, 3, padding='same', activation='relu'),\n",
        "    layers.MaxPooling2D(),\n",
        "    layers.Conv2D(64, 3, padding='same', activation='relu'),\n",
        "    layers.MaxPooling2D(),\n",
        "    layers.Flatten(),\n",
        "    layers.Dense(128, activation='relu'),\n",
        "    layers.Dense(num_classes)  # Output logits\n",
        "])\n",
        "\n",
        "# Compile the model\n",
        "model.compile(\n",
        "    optimizer='adam',\n",
        "    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),  # use 'from_logits=True' for raw outputs\n",
        "    metrics=['accuracy']\n",
        ")\n",
        "\n",
        "\n",
        "history = model.fit(\n",
        "    train_ds,\n",
        "    validation_data=test_ds,\n",
        "    batch_size=32,\n",
        "    epochs=30\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "id": "yIxaefM8yHwQ",
        "outputId": "40d435c7-9041-449a-c8e9-01072613fff4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'keras' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1-243836689.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# Build the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m model = keras.Sequential([\n\u001b[0m\u001b[1;32m      7\u001b[0m     \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mInput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mIMG_HEIGHT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mIMG_WIDTH\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# Input layer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mlayers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRandomFlip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"horizontal\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;31m# Added data augmentation layers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'keras' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: print the test accuracy\n",
        "\n",
        "loss, accuracy = model.evaluate(test_ds)\n",
        "print('Test accuracy :', accuracy)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3w_TAmDu4d_9",
        "outputId": "7a76207d-a27b-474f-d4de-8a84acc85477"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.7057 - loss: 1.0275\n",
            "Test accuracy : 0.7125340700149536\n"
          ]
        }
      ]
    }
  ]
}